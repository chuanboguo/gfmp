---
title: Shortraker rockfish (*Sebastes borealis*) example operating model
author: ''
date: ''
output: html_document
---

Echave, K.B., Shotwell, S.K., and Hulson, P.-J.F. 2015. Assessment of the shortraker rockfish stock in the Gulf of Alaska. In Stock assessment and fishery evaluation report for the groundfish resources of the Gulf of Alaska. North Pacific Fishery Management Council, 605 W 4 th Ave, Suite 306, Anchorage AK 99501. pp. 975–1012.

Love, M.S. 2011. Certainly More Than You Want to Know about the Fishes of the Pacific Coast: A Postmodern Experience. Really Big Press.

Love, M.S., Yoklavich, M., and Thorsteinson, L.K. 2002. The Rockfishes of the Northeast Pacific. University of California Press.

Thorson, J.T., Jensen, O.P., and Zipkin, E.F. 2014. How variable is recruitment for exploited marine fishes? A hierarchical model for testing life history theory. Can. J. Fish. Aquat. Sci. 71(7): 973–983. doi: 10.1139/cjfas-2013-0645.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = TRUE,
  autodep = TRUE
)
```

```{r, message=FALSE, warning=FALSE, results='hide'}
library(DLMtool)
library(gfplot)
library(rfishbase)
# library(PBSmptools)
# devtools::load_all("../pbs-mptools")
# options(mc.cores = parallel::detectCores())
n_iters <- 50

# Set up Shortraker rockfish data - d will contain the data for commercial samples, survey, and catch
if(!exists(d)){
  fetch_data("shortraker rockfish")
}
d <- load_data("shortraker rockfish")
```

## Stock parameters

Let's start with the generic rockfish model and modify from there.

```{r}
stock <- DLMtool::Rockfish
```

@love2002 note that shortraker rockfish have been aged at 157 years. Shortraker rockfish have been aged to by DFO in the past. Let's pull the maximum age from GFBio:

*Not necessarily reliable because ageing very uncertain; probably should look at some upper quantile or take value from elsewhere*

```{r}
# (gfbio_maxage <- max(PBSmptools::short_age_length$age))
# stock@maxage <- gfbio_maxage
```

R0, the magnitude of unfished recruitment is fixed to an arbitrary value since it scales the simulated numbers:

```{r}
stock@R0
```

@echave2015 estimate natural mortality in the Gulf of Alaska to be 0.03. Let's place some relatively wide bounds around that. 

```{r}
stock@M <- c(0.02, 0.10)
```

Inter-annual variability in natural mortality rate expressed as a uniform distribution of a CV. We are unlikely to find any data on this and need to take a best guess. Leaving this at the default:

```{r}
stock@Msd
```

Mean temporal trend in natural mortality rate, expressed as a percent change in M per year. Unlikely to find any data on this. Set it to range between -0.25% and 0.25% per year.

```{r}
stock@Mgrad <- c(-0.0025, 0.0025)
```

Fraction of discarded fish that die. For shortraker, and the most rockfish, this value should be at or near 100%.

```{r}
stock@Fdisc <- c(1, 1)
```

Steepness of the stock recruit relationship (uniform distribution). TODO take from Robyn's work?

```{r}
stock@h
```

Maximum length: @love2002 list the maximum size as 120 cm. They also note that in the Gulf of Alaska fish smaller than 35 cm are rarely found. Fishbase lists a maximum length of 108 cm. However, we can fit von Bertalanffy growth curves to the length-age data in GFBio. 

Because we are providing our own samples from the model posterior, we provide these values using the custom parameters functionality in the `cpar` slot of the final operating model. 

```{r}
short_age_length <- 
# short_age_length <- PBSmptools::get_age_length("Sebastes borealis", "West Coast Haida Gwaii")
# short_age_length <- PBSmptools::short_age_length
# set.seed(123)
# vb_model <- PBSmptools::fit_vb(
#   age = short_age_length$age, 
#   length = short_age_length$length,
#   control = list(adapt_delta = 0.9), iter = 800, cores = 1, chains = 3)
# bayesplot::mcmc_combo(as.matrix(vb_model), pars = c("k", "linf", "t0", "sigma"))
# vb_samples <- PBSmptools::sample_posterior_vb(vb_model, n = n_iters)
# pairs(vb_samples)

# # TODO move this plotting code to the package:
# e <- rstan::extract(vb_model)
# ages <- seq(0, 150, length.out = 200)
# p <- matrix(nrow = length(PBSmptools::short_age_length$age), ncol = 150)
# samples <- sample(seq_len(length(e$linf)), size = ncol(p))
# for (i in 1:nrow(p)) {
#   for (j in seq_len(ncol(p))) {
#     p[i, j] <- e$linf[samples[j]] * (1 - exp(-e$k[samples[j]] * (ages[i] - e$t0[samples[j]])))
#   }
# }
# p <- reshape2::melt(p)
# p$age <- ages[p$Var1]
# p$sex <- 1
# library(ggplot2)
# ggplot(PBSmptools::short_age_length, aes(age, length, colour = as.factor(sex))) +
#   geom_point() +
#   geom_line(aes(y = value, group = Var2), data = p, colour = "black", alpha = 0.05)
```

Let's try fitting VB models that permit a specified level of aging measurement error. We will specify this level of measurement error with a CV.

<!-- ```{r} -->
<!-- library(rstan) -->
<!-- rstan_options(auto_write = TRUE) -->
<!-- short_age_length <- PBSmptools::short_age_length -->

<!-- cv_ages <- c(0, 0.2) -->
<!-- e <- list() -->
<!-- g <- list() -->
<!-- vb_fit <- list() -->
<!-- for (ii in seq_along(cv_ages)) { -->
<!--   if (cv_ages[[ii]] == 0) { -->
<!--     vb_fit[[ii]] <- stan("analysis/vb_gamma.stan", -->
<!--       data =  -->
<!--         list(age = short_age_length$age, -->
<!--           length = short_age_length$length, -->
<!--           N = length(short_age_length$age)), -->
<!--       control = list(adapt_delta = 0.99, max_treedepth = 25),  -->
<!--       iter = 2000, cores = 1, chains = 4, -->
<!--       pars = c("k", "linf", "cv_length", "t0")) -->
<!--   } else { -->
<!--     vb_fit[[ii]] <- stan("analysis/vb_aging_error_gamma.stan", -->
<!--       data =  -->
<!--         list(age = short_age_length$age, -->
<!--           length = short_age_length$length, -->
<!--           N = length(short_age_length$age), -->
<!--           cv_age = cv_ages[ii]), -->
<!--       control = list(adapt_delta = 0.99, max_treedepth = 25),  -->
<!--       iter = 2000, cores = 1, chains = 4, -->
<!--       pars = c("k", "linf", "cv_length", "t0")) -->
<!--   } -->
<!--   e[[ii]] <- rstan::extract(vb_fit[[ii]]) -->
<!-- } -->
<!-- ages <- seq(0, max(PBSmptools::short_age_length$age), length.out = 200) -->
<!-- for (ii in seq_along(cv_ages)) { -->
<!--   p <- matrix(nrow = length(PBSmptools::short_age_length$age), ncol = 150) -->
<!--   samples <- sample(seq_len(length(e[[ii]]$linf)), size = ncol(p)) -->
<!--   for (i in 1:nrow(p)) { -->
<!--     for (j in seq_len(ncol(p))) { -->
<!--       p[i, j] <- e[[ii]]$linf[samples[j]] *  -->
<!--         (1 - exp(-e[[ii]]$k[samples[j]] * (ages[i] - e[[ii]]$t0[samples[j]]))) -->
<!--     } -->
<!--   } -->
<!--   p <- reshape2::melt(p) -->
<!--   p$age <- ages[p$Var1] -->
<!--   p$sex <- 1 -->
<!--   library(ggplot2) -->
<!--   g[[ii]] <-  -->
<!--     ggplot(PBSmptools::short_age_length, aes(age, length, colour = as.factor(sex))) + -->
<!--     geom_point() + -->
<!--     geom_line(aes(y = value, group = Var2), data = p, colour = "black", alpha = 0.05) + -->
<!--     guides(colour = FALSE) -->
<!-- } -->
<!-- # gridExtra::grid.arrange(g[[1]], g[[2]], g[[3]], g[[4]]) -->

<!-- # devtools::install_github("seananderson/stanhelpers") -->
<!-- out <- plyr::llply(vb_fit, stanhelpers::extract_df, output = "long_df") -->
<!-- for (i in seq_along(cv_ages)) out[[i]]$cv_age <- cv_ages[[i]] -->
<!-- out <- dplyr::bind_rows(out) -->
<!-- out <- dplyr::filter(out, variable != "lp__") -->
<!-- ggplot(out, aes(value)) + geom_histogram() +  -->
<!--   facet_grid(cv_age~variable, scales = "free") -->

<!-- ggplot(out, aes(as.factor(cv_age), value)) +  -->
<!--   geom_violin(draw_quantiles = c(0.5)) +  -->
<!--   facet_wrap(~variable, scales = "free_y") -->

<!-- cv <- 0.2 -->
<!-- ages <- seq(1, 150, length.out = 1000) -->
<!-- ages_w_error <- rgamma(n = length(ages), shape = 1/cv^2, rate = 1/(ages * cv^2)) -->
<!-- par(xpd = NA) -->
<!-- plot(ages, ages_w_error, yaxs = "i", ylim = c(0, max(ages_w_error)*1.01),  -->
<!--   col = "#00000040", pch = 19) -->
<!-- lines(ages, ages) -->

<!-- cv <- 0.2 -->
<!-- g[[2]] + geom_segment(data = dplyr::mutate(PBSmptools::short_age_length,  -->
<!--   l = qgamma(0.025, shape = 1/cv^2, rate = 1/(age * cv^2)), -->
<!--   u = qgamma(0.975, shape = 1/cv^2, rate = 1/(age * cv^2))), -->
<!--   aes(x = l, xend = u, y = length, yend = length), alpha = 0.1, -->
<!--   colour = "black") + -->
<!--   theme_light() + -->
<!--   coord_cartesian(xlim = c(0, 160)) -->
<!-- ggsave("analysis/short-vb-ex.pdf", width = 6, height = 5) -->

<!-- # e <- stanhelpers::extract_df(vb_fit[[2]], output = "wide_df") %>%  -->
<!-- #   dplyr::select(k, linf, t0) -->
<!-- # GGally::ggpairs(e) -->

<!-- vb_samples <- extract(e[[2]]) -->
<!-- ``` -->


```{r}
# short@Linf
# short@K
# short@t0
```

Coefficient of variation of length-at-age (assumed constant for all age classes).

```{r}
stock@LenCV
```

Inter-annual variability in growth rate expressed as a log normal standard deviation. TODO

```{r}
stock@Ksd
```

Mean temporal trend in growth parameter K, expressed as a percentage change in K per year. We leave it at the default of ranging between -0.25% and 0.25% per year.

```{r}
stock@Kgrad
```

Inter-annual variability in maximum length.

```{r}
stock@Linfsd
```

Range of annual percent change in L infinity.

```{r}
stock@Linfgrad
```

Mean temporal trend in log-normal recruitment deviations. Need to look into exactly what this corresponds to. Trend over what period of time? 

```{r}
#stock@recgrad
```

Autocorrelation in recruitment deviations rec(t) = AC*rec(t-1)+(1-AC)*sigma(t). The default is a wide range, `r stock@AC`. @thorson2014 find that autocorrelation of recruitment deviations is probably around 0.45. Barring better information in the literature, let's leave the default wide range:

```{r}
stock@AC
```

Length-weight parameter alpha. This parameter is known as the condition factor. Weight = a * Length ^ b. Let's use the mean value from Fishbase.

Alternatively, we could use the value cited in @love2002 (a = 0.0098, b = 3.130), which is based on Martin (1997) (reference found within @love2002).

Values within @love2011 are even lower. 

**2018-03-29: should use values from the model fits to our data in other document**

```{r}
lw <- rfishbase::length_weight("Sebastes borealis")
lw$a
mean(lw$a)
stock@a <- mean(lw$a)
```

Length-weight parameter beta. This parameter is known as the allometric growth parameter. Again, let's use the mean value from Fishbase.

```{r}
lw$b
mean(lw$b)
stock@b <- mean(lw$b)
```

Length-at-50 percent maturity. @love2002 p. 136 list length at 50% maturity of 45 cm for both sexes off of West Vancouver Island and this value may be cited from @dfo1999. @conrath2017 recently sampled 357 shortraker rockfish in the Gulf of Alaska. They note that they doubt there is substantial regional variation in these maturity parameters for these species. They calculated L50 as 49.4 cm with a 95% CI of 47.9--50.8 cm.

**2018-03-29: should use values from the model fits to our data in other document**

```{r}
stock@L50 <- c(48, 51)
```

Length increment between the length at 50% maturity and 95% maturity. Figure 5 in @conrath2017 shows the proportion mature by length. 95% are mature by approximately 60 cm. 60 cm - 49 cm = 11cm. So, let's set `L50_95` to approximately 11 cm with a small uncertainty buffer.

**2018-03-29: should use values from the model fits to our data in other document**

```{r}
stock@L50_95 <- c(9, 13)
```

Current level of stock depletion (Bcurrent/Bunfished). We need to put some thought into this still: TODO

```{r}
stock@D
```

Process error, the CV of lognormal recruitment deviations. @thorson2014 finds the mean SD of recruitment deviations in log space to be ballpark 0.6--0.8. Need to look for data in other assessments of similar rockfish species or of shortraker in other regions. 

```{r}
stock@Perr <- c(0.2, 0.8)
```

Period for cylical recruitment pattern in years. Leave empty to ignore.

```{r}
stock@Period
```

Amplitude in deviation from long-term average recruitment during recruitment cycle, both positive and negative (uniform distribution). E.g., a range from 0 to 0.5 means recruitment decreases or increases by up to 50% each cycle. Leaving empty to ignore.

```{r}
stock@Amplitude
```

The fraction of the unfished habitat in area 1 of 2. Area 1 represents habitat outside of the range of fishing. We put 10% of individuals in area 1 and 90% in area 2. TODO

```{r}
stock@Frac_area_1 <- c(0.1, 0.1)
```

The probability of inviduals in area 1 remaining in area 1 over the course of one year. TODO 

```{r}
stock@Prob_staying <- c(0.85, 0.95)
```

"Size_area_1: to account for spatial targeting the toolkit has a slot for the size of each area (to inform density of fish), however this feature is under development so it is set to be the same as the habitable area (Frac_area_1), 0.1." <- TODO text from Tom

```{r}
stock@Size_area_1 <- stock@Frac_area_1
```

```{r}
(stock@Source <- paste(c("A modification of a rockfish model based on",
  stock@Source), collapse = " "))
```

Let's combine the stock, fleet, observation, and implementation objects into a complete operating model:

```{r}
# avail("Obs")
# avail("Imp")
# avail("Fleet")
# avail("Output")
short_om <- new('OM', stock, Generic_fleet, Imprecise_Unbiased, Perfect_Imp)
MPs <- c("DCAC", "DCAC_40", "DCAC_ML", "DBSRA_40", 
  "DCAC4010", "DBSRA4010", "DBSRA", "DBSRA_ML",
  "CC1", "CC4")
short_om@nsim <- n_iters
```

Let's now add the custom parameters:

```{r}
short_om@cpars$Linf <- vb_samples$linf
short_om@cpars$K <- vb_samples$k
short_om@cpars$t0 <- vb_samples$t0
```

There are a variety of built-in plotting functions to examine the components of the operating model:

```{r, eval=FALSE}
plotStock(short_om, nsamp = 3, incVB = FALSE)

pdf("analysis/short-ex-stock.pdf", width = 10, height = 12)
plotStock(short_om, nsamp = 3, incVB = FALSE)
dev.off()
```

```{r, eval=FALSE}
plotSelect(short_om, sim = 1) # one sample, others will look different
```

```{r, eval=FALSE}
plotFleet(Generic_fleet, Stock = stock)

pdf("analysis/short-ex-fleet.pdf", width = 10, height = 12)
plotFleet(Generic_fleet, Stock = stock)
dev.off()
```

```{r, eval=FALSE}
plotImp(Perfect_Imp)
```

```{r, eval=FALSE, fig.height=9}
plotObs(Imprecise_Unbiased)
```

Or we can create all the plots at once:

```{r, fig.height=9, fig.width=10}
plot(short_om)

pdf("analysis/short-ex-om-all.pdf", width = 10, height = 12)
plot(short_om)
dev.off()
```

Now we can run closed-loop simulation:

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', eval=FALSE}
DLMtool::setup(cpus = parallel::detectCores())
short_mse <- runMSErobust(OM = short_om, MPs = MPs, name = "analysis/short-mse")
```

And examine the output. 

```{r}
short_mse <- readRDS("analysis/short-mse.rdata")
plot(short_mse)
# DFO_hist(short_mse)
DFO_plot(short_mse)
DFO_plot2(short_mse)
```

```{r, fig.height=9, warning=FALSE}
DFO_proj(short_mse)
DFO_hist(short_om)
d <- DFO_plot2(short_mse)

ggplot(d, aes(B50/100, LTY/100, label = MP, colour = Satisfice)) +
  geom_vline(xintercept = 0.5, col = "grey50") +
  geom_hline(yintercept = 0.5, col = "grey50") +
  geom_text() + ggsidekick::theme_sleek() +
  scale_colour_manual(values = c("grey60", "black")) +
  guides(colour = FALSE) +
  xlab(expression(Pr(B>0.5~B[MSY]))) +
  ylab(expression(Pr(yield>0.5~yield~at~F[MSY]))) +
  xlim(0, 1) + ylim(0, 1)
ggsave("analysis/short-obj-plot.pdf", width = 5, height = 5)
```

```{r}
yend <- max(short_mse@proyears)
ffmsy <- short_mse@F_FMSY[,,yend]
bbmsy <- short_mse@B_BMSY[,,yend]

ffmsy <- reshape2::melt(ffmsy) %>% 
  dplyr::rename(iter = Var1, mp = Var2, ffmsy = value)
bbmsy <- reshape2::melt(bbmsy) %>% 
  dplyr::rename(iter = Var1, mp = Var2, bbmsy = value)

d <- dplyr::inner_join(ffmsy, bbmsy)

n <- data.frame(mp = 1:length(short_mse@MPs), mp_name = short_mse@MPs)
# n$mp_name <- paste(1:nrow(n), n$mp_name)

d <- dplyr::left_join(d, n)

ggplot(dplyr::filter(d, !mp_name %in% c("CC4", "DBSRA_ML")), aes(bbmsy, ffmsy)) +
  geom_vline(xintercept = c(0.4, 0.8), alpha = 0.2) +
  geom_hline(yintercept = 1, alpha = 0.2) +
  geom_density_2d(aes(colour = ..level..), bins = 5) +
  viridis::scale_colour_viridis() +
  ggsidekick::theme_sleek() +
  facet_wrap(~mp_name) +
  ylim(0, 3.5) + xlim(0, 3.5) +
  geom_point(alpha = 0.2) +
  labs(colour = "Prob. density", x = expression(B/B[MSY]),
    y = expression(F/F[MSY])) +
  guides(colour = FALSE)
ggsave("analysis/short-bbmsy-ffmsy-multi.pdf", width = 8, height = 8)

ggplot(dplyr::filter(d, mp_name == c("DBSRA")), aes(bbmsy, ffmsy)) +
  geom_vline(xintercept = c(0.4, 0.8), alpha = 0.2) +
  geom_hline(yintercept = 1, alpha = 0.2) +
  geom_density_2d(aes(colour = ..level..), bins = 5) +
  viridis::scale_colour_viridis() +
  ggsidekick::theme_sleek() +
  facet_wrap(~mp_name) +
  ylim(0, 2.2) + xlim(0, 2.2) +
  geom_point(alpha = 0.2) +
  labs(colour = "Prob. density", x = expression(B/B[MSY]),
    y = expression(F/F[MSY])) +
  guides(colour = FALSE)
ggsave("analysis/short-bbmsy-ffmsy-dbsra.pdf", width = 4, height = 4)
```

```{r}
library(dplyr)
ffmsy <- short_mse@F_FMSY[,,] %>% reshape2::melt() %>% 
  dplyr::rename(iter = Var1, mp = Var2, ffmsy = value, year = Var3)
bbmsy <- short_mse@B_BMSY[,,] %>% reshape2::melt() %>% 
  dplyr::rename(iter = Var1, mp = Var2, bbmsy = value, year = Var3)
d <- dplyr::inner_join(ffmsy, bbmsy)
d <- dplyr::left_join(d, n)
d <- reshape2::melt(d, id.vars = c("iter", "mp", "year", "mp_name"))
levels(d$variable) <- c(expression(F/F[MSY]), expression(B/B[MSY]))

d_median <- d %>% group_by(mp_name, year, variable) %>% 
  summarise(median_value = median(value)) %>% 
  mutate(iter = NA)

fudge <- 3.5
d$over <- FALSE
d$over[d$value > fudge] <- TRUE
d$value[d$over] <- fudge
d_median$median_value[d_median$median_value > fudge] <- fudge

d_last <- dplyr::filter(d, year == max(year)) %>% 
  select(-over, -year) %>% 
  rename(last_value = value)
d <- inner_join(d, d_last)

library(RColorBrewer)
cols <- brewer.pal(3, "RdBu")
cols[2] <- "grey80"

plot_timeseries <- function(dat, dat_median, title = "", 
  ylim = c(0, fudge), cols, values, labels = TRUE, yaxis = TRUE) {
  g <- ggplot(dat, aes_string("year", "value", group = "iter")) + 
    geom_line(aes_string(colour = "last_value"), alpha = 0.3) +
    facet_grid(mp_name~., labeller = label_parsed) + 
    ggsidekick::theme_sleek() +
    scale_colour_gradientn(colours = cols, 
      values = values) +
    geom_line(data = dat_median, aes_string("year", "median_value"), 
      colour = "black", lwd = 1.5) +
    ylim(ylim[1], ylim[2]) +
    ggtitle(title) +
    guides(colour = FALSE) +
    ylab("Value") + xlab("Year")
  
  if (!labels) 
    g <- g + theme(strip.background = element_blank(),
      strip.text.y = element_blank())
  
  if (!yaxis) 
    g <- g + theme(axis.text.y = element_blank())
  g
}

cols <- brewer.pal(5, "RdBu")
cols <- c(cols[1], "grey50", cols[length(cols)])
set.seed(42)
d_ <- filter(d, iter %in% sample(seq_len(max(d$iter)), 30))

# plot_timeseries(d, d_median,
# cols = cols, values = scales::rescale(c(0, 0.8, 1.2, fudge)))

mps <- c("CC1", "DBSRA")
p1 <- plot_timeseries(filter(d_, variable == "B/B[MSY]", mp_name %in% mps),
  filter(d_median, variable == "B/B[MSY]", mp_name %in% mps), expression(B/B[MSY]),
  cols = cols, values = scales::rescale(c(0, 0.8, 1.2, fudge)),
  labels = TRUE)
p2 <- plot_timeseries(filter(d_, variable == "F/F[MSY]", mp_name %in% mps),
  filter(d_median, variable == "F/F[MSY]", mp_name %in% mps), expression(F/F[MSY]),
  cols = cols, values = scales::rescale(c(fudge, 1.2, 0.8, 0)),
  labels = TRUE, yaxis = TRUE)
gridExtra::grid.arrange(p1, p2, ncol = 2)

pdf("analysis/short-bbmsy-ffmsy-ts-ex.pdf", width = 6, height = 3.5)
gridExtra::grid.arrange(p1, p2, ncol = 2)
dev.off()

mps <- unique(d_$mp_name)
mps <- mps[-which(mps %in% c("CC4"))]
p1 <- plot_timeseries(filter(d_, variable == "B/B[MSY]", mp_name %in% mps),
  filter(d_median, variable == "B/B[MSY]", mp_name %in% mps), expression(B/B[MSY]),
  cols = cols, values = scales::rescale(c(0, 0.8, 1.2, fudge)),
  labels = TRUE)
p2 <- plot_timeseries(filter(d_, variable == "F/F[MSY]", mp_name %in% mps),
  filter(d_median, variable == "F/F[MSY]", mp_name %in% mps), expression(F/F[MSY]),
  cols = cols, values = scales::rescale(c(fudge, 1.2, 0.8, 0)),
  labels = TRUE, yaxis = TRUE)
# gridExtra::grid.arrange(p1, p2, ncol = 2)

pdf("analysis/short-bbmsy-ffmsy-ts-ex-all.pdf", width = 6, height = 6)
gridExtra::grid.arrange(p1, p2, ncol = 2)
dev.off()

mps <- unique(d_$mp_name)
mps <- mps[-which(mps %in% c("CC4"))]
p1 <- plot_timeseries(filter(d_, variable == "B/B[MSY]", mp_name %in% mps),
  filter(d_median, variable == "B/B[MSY]", mp_name %in% mps), expression(B/B[MSY]),
  cols = cols, values = scales::rescale(c(0, 0.8, 1.2, fudge)),
  labels = TRUE)
p2 <- plot_timeseries(filter(d_, variable == "F/F[MSY]", mp_name %in% mps),
  filter(d_median, variable == "F/F[MSY]", mp_name %in% mps), expression(F/F[MSY]),
  cols = cols, values = scales::rescale(c(fudge, 1.2, 0.8, 0)),
  labels = TRUE, yaxis = TRUE)
# gridExtra::grid.arrange(p1, p2, ncol = 2)

pdf("analysis/short-bbmsy-ffmsy-ts-ex-all.pdf", width = 6, height = 6)
gridExtra::grid.arrange(p1, p2, ncol = 2)
dev.off()

#########3
ffmsy <- short_mse@F_FMSY[,,] %>% reshape2::melt() %>% 
  dplyr::rename(iter = Var1, mp = Var2, ffmsy = value, year = Var3)
bbmsy <- short_mse@B_BMSY[,,] %>% reshape2::melt() %>% 
  dplyr::rename(iter = Var1, mp = Var2, bbmsy = value, year = Var3)
d <- dplyr::inner_join(ffmsy, bbmsy)
d <- dplyr::left_join(d, n)
d <- reshape2::melt(d, id.vars = c("iter", "mp", "year", "mp_name"))
levels(d$variable) <- c(expression(F/F[MSY]), expression(B/B[MSY]))

d_sum <- d %>% group_by(mp_name, year, variable) %>% 
  summarise(median_value = median(value),
    l = quantile(value, probs = 0.75),
    u = quantile(value, probs = 0.25),
    ll = quantile(value, probs = 0.95),
    uu = quantile(value, probs = 0.05))


# d_sum$median_value[d_sum$median_value > fudge] <- fudge
# d_sum$u[d_sum$u > fudge] <- fudge
# d_sum$l[d_sum$l > fudge] <- fudge

d <- inner_join(d, d_last)

d$last_value[d$last_value > fudge] <- fudge
# d$value[d$value > fudge] <- fudge


cols <- brewer.pal(5, "RdBu")
cols <- c(cols[1], "grey50", cols[length(cols)])
g <- ggplot(d_sum, aes_string("year", "median_value")) + 
  facet_grid(mp_name~variable, labeller = label_parsed) + 
  geom_ribbon(aes(ymin = ll, ymax = uu), fill = "grey90") +
  geom_ribbon(aes(ymin = l, ymax = u), fill = "grey70") +
  ggsidekick::theme_sleek() +
  geom_line(lwd = 1.5) +
  coord_cartesian(ylim = c(0, 4)) +
  guides(colour = FALSE) +
  xlim(25, 50) +
  geom_hline(yintercept = 1, 
    col = "black", lty = 2) +
  ylab("Value") + xlab("Year")
  # geom_line(data = filter(d, iter %in% c(1:10)), 
  #   aes(y = value, group = iter),
  #   alpha = 0.4, colour = "blue")
  # scale_colour_gradientn(colours = cols, 
  #     values = c(0, 0.8, 1.2, 3))
  # viridis::scale_colour_viridis()
g


```



```{r}
# get_contour <- function(df, x_variable, y_variable, prob = 0.8, n = 200, ...) {
#   x <- df[, c(x_variable, y_variable)] %>%
#     data.matrix()  %>%
#       coda::mcmc() %>%
#         emdbook::HPDregionplot(prob = prob, n = n, ...)
# 
#   for (j in length(x)) x[[j]]$contour_group <- j
# 
#   x <- x %>% lapply(as.data.frame) %>%
#     dplyr::bind_rows()
# 
#   x <- dplyr::mutate(x, contour_group =
#     ifelse(!is.na(contour_group), contour_group, 1))
# 
#   x[,c("x", "y", "contour_group")]
# }
# 
# plot_polygons <- function(polygon_data, x_column, y_column,
#   xlab = "Variance", ylab = "Mean", prob = 0.75,
#   grouping = c("diversity_group"), ...) {
#   contours <- plyr::ddply(polygon_data, grouping,
#     function(x) get_contour(x, x_column, y_column, prob = prob))
# # We need unique IDs in case there are multiple polygons for one group
#   contours$id <- paste(contours$contour_group, contours[,grouping[1]])
#   polygon_data <- dplyr::mutate(polygon_data, id = 1)
# 
#   p <- contours %>%
#     ggplot(aes_string("x", "y", group = "id", color = grouping[1],
#       fill = grouping[1]))
# 
#   p <- p + geom_point(data = polygon_data, aes_string(x_column, y_column),
#     alpha = 0.1)
#   p <- p + geom_polygon(alpha = 0.3) +
#     viridis::scale_fill_viridis() + 
#     viridis::scale_colour_viridis() +
#     theme_bw() +
#     xlab(xlab) + ylab(ylab)
# 
#   if (length(grouping) > 1)
#     p <- p + facet_wrap(grouping[2])
# 
#   print(p)
#   invisible(contours)
# }
# 
# plot_polygons(data.frame(bbmsy = bbmsy, ffmsy = ffmsy, group = 1), "bbmsy", "ffmsy", grouping = "group")

```

VOI:

```{r}
pdf("analysis/short-voi-om-b.pdf", width = 8, height = 8)
VOIplot(short_mse, Par="OM", nMP=5, YVar="B")
dev.off()
```


# References
