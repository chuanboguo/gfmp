# BEST PRACTICES FOR IMPLEMENTING A MANAGEMENT PROCEDURE APPROACH

A critical first step in any decision-making processes is defining the decision context [@gregory2012]. This is particularly important in complex decision-making approaches such as MSE, where it is necessary to clarify which decisions that need to be made and, importantly, the plan for reaching a decision([^1]). Some key questions to be answered at this step are listed in Step 1 below. Following this step, implementation of the MSE also follows a series of stages. @punt2016 reviewed best practices for MSE and identified five key steps in the process (described in Steps 2-6 below). In large part, DLMtool has been designed to follow these steps, illustrated in Figure \@ref(fig:fig-dlmtool) [@carruthers2018].  

Importantly, decision-makers, stakeholders and other interested parties (First Nations, NGOs, academics) should be engaged throughout the process, particularly in defining the decision context, setting objective and performance metrics and selection of MPs [e.g., @cox2008a].

*TO DO: figure out how to cite the landmark report. We are not allowed to cite unpublished reports in CSAS docs (I don't think), but we could include it in a footnote. I tried doing a footnote but it didn't work. Anyway, here is the ref    ([^1]: Cox, S.P., and Benson, A.J. Unpublished. Roadmap to more sustainable Pacific herring fisheries in Canada: a step-by-step guide to the management strategy evaluation approach. Prepared for Pelagics Resource Management, Fisheries and Oceans Canada Pacific Region.) *


### 1. Define the decision context


Defining the decision context . Key questions to guide definition of the decision context for MSE include:

* What is the the exact decision to be made?

* What is the timeframe for making the decision?

* What are the boundaries on the project and decision?

* What are specific roles and responsibilities of those involved, who may include science, management, First Nations, industry, academia and NGOs? 

* How will the final decision will be made? For example, it may be necessary to rank or weight objectives if there is large conflict in performance against objectives.


### 2. Selection of objectives and performance metrics

Before running any MSE analyses, clear objectives and the performance metrics that measure them must be identified. Ideally, these should be developed with the participation of decision-makers, stakeholders and other interested parties. Objectives may initially be high level and "strategic" (e.g., achieve sustainable fisheries, maintain economic prosperity, maintain cultural access) but these must be converted into operational "tactical" objectives that can be expressed as quantitative performance metrics [@punt2016]. Fully quantified objectives include a metric, the desired probability of success, and a time frame to achieve the objective (e.g., probability that stock size is below LRP is less than 5% over 50 years). Since the underlying system represented by the OM is known exactly, a wide range of biological and economic metrics is available [@carruthers2018]. 

@punt2016 note that care should be taken to avoid having too many performance metrics, which could make the final decision process too complex. They also note that performance metrics should be chosen so they can be understood by decision-makers and participants.

### 3. Selection of uncertainties/specification of operating models

Uncertainties inherent in the underlying system are represented in the OM. These may be related to the biology of the stock (e.g., growth, natural mortality, recruitment, migration), the dynamics of the fleet (e.g., targeting behaviour, selectivity of the fishing gear), the observation process (e.g., bias or imprecision in survey data or age/length composition data), and the implementation process (e.g., exceeding catch limits) [@carruthers2018]. 

Some of this uncertainty (e.g., range of values of natural mortality or other parameters) may be captured in a single OM. However, it is unlikely that the full range of uncertainties thought to influence the system can be captured in one model. Therefore, best practice recommends dividing MSE trials into a "reference" set, represented by a baseline OM, and a "robustness" set, represented by other plausible OM formulations that represent alternative structural hypotheses (e.g., time-varying natural mortality vs constant natural mortality, climate-driven recruitment, or alternative depletion states of the stock) [@rademeyer2007]. @punt2016 provide a list of factors whose uncertainty commonly has a large impact on MSE performance (their Table 3).


### 4. Identification of candidate management procedures


### 5. Simulation of the application of the management procedures


### 6. Presentation of results and selection of a management procedure




# METHODS 

Text text text

## HOW THE PROPOSED APPROACH ADOPTS THESE BEST PRACTICES

* Define decision context
    -  give examples throughout including what we propose as starting points

* Define specific objectives and performance indicator
    - suggested starting points for objectives and performance indicators across most stocks this will apply to
    - justify these objectives and performance indicators based on PA framework, literature, and precedent in Canada and elsewhere

* Identify management procedure options
    - summarize candidates that we have narrowed it down to
    - reference the detailed appendix

* Define a set of operating models that capture main uncertainties
    - what are the common main uncertainties that we should make sure to capture
    - what are potential sources of uncertainty that by default we suggest should not be focused on (but could be included on a stock by stock basis)
    - the idea behind robustness sets; example robustness sets
    - consider weights based on relative plausibility (and why by default we suggest avoiding this complexity)

* Simulate and compute performance indicators
    - proposed software (DLMtool)
    	- why use it?
    	- what it can do?
    	- note that the approach is software agnostic
    	- examples of where it has been used
    - how many iterations: checking for convergence of MP selection

* Rank overall MP performance
    - suggested figures and table
    - explain satisficing
    - describe potential default satisficing rules

* Apply the MP to real fishery
    - same MP; real data 
    - reinforce that MP needs to be followed exactly as defined to meet desired objectives
    - if adjustments need to be made this suggests a need to revisit the objectives
    

