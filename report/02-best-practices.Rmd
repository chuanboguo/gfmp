# BEST PRACTICES FOR IMPLEMENTING A MANAGEMENT PROCEDURE APPROACH

A critical first step in any decision-making processes is defining the decision context [@gregory2012]. This is particularly important in complex decision-making environments such as MSE, where it is necessary to clarify the decision that needs to be made and, importantly, the planned approach for reaching a decision. Some key questions to be answered at this stage are provided under Step 1 below. @punt2016 reviewed best practices for MSE and identified five key steps in the process (described under Steps 2--6 below). In large part, DLMtool has been designed to follow these steps, illustrated in Figure \@ref(fig:mse-chart) [@carruthers2018].

(ref:fig-mse-chart) The steps of the MSE process following @punt2016 as implemented in DLMtool. Adapted from @carruthers2018.

```{r mse-chart, fig.cap="(ref:fig-mse-chart)", out.width="6.3in"}
knitr::include_graphics(here::here("report/figure/mse-chart.pdf"))
```

Importantly, decision-makers, stakeholders, and other interested parties (e.g., First Nations, Nongovernmental Organizations [NGOs], and academics) should be engaged throughout the process, particularly in defining the decision context, setting objectives and performance metrics and selection of MPs [e.g., @cox2008a].

<!-- TODO: figure out how to cite the landmark report. We are not allowed to cite unpublished reports in CSAS docs (I don't think), but we could include it in a footnote. I tried doing a footnote but it didn't work. Anyway, here is the ref    ([^1]: Cox, S.P., and Benson, A.J. Unpublished. Roadmap to more sustainable Pacific herring fisheries in Canada: a step-by-step guide to the management strategy evaluation approach. Prepared for Pelagics Resource Management, Fisheries and Oceans Canada Pacific Region.) -->

## STEP 1. DEFINE THE DECISION CONTEXT

Key questions to guide defining the decision context for MSE include:

* What is the the exact decision to be made?

* What is the timeframe for making the decision?

* What are the boundaries on the project and decision?

* What are specific roles and responsibilities of those involved, who may include science, management, First Nations, industry, academia, and NGOs? 

* How will the final decision be made? For example, it may be necessary to rank or weight objectives if there is large conflict in performance against objectives.

## STEP 2. SELECTION OF OBJECTIVES AND PERFORMANCE METRICS

Before running any MSE analyses, clear objectives and the performance metrics that measure them must be identified. Ideally, these should be developed with the participation of decision-makers, stakeholders, and other interested parties. Objectives may initially be high level and "strategic" (e.g., achieve sustainable fisheries, maintain economic prosperity, maintain cultural access) but these must be converted into operational "tactical" objectives that can be expressed as quantitative performance metrics [@punt2016]. Fully quantified objectives include a metric, the desired probability of success, and a time frame to achieve the objective (e.g., the probability that the stock size is below the LRP is less than 5% over 50 years). Since the underlying system represented by the OM is known exactly, a wide range of biological and economic metrics can be calculated [@carruthers2018]. However, having too many performance metrics can make the final decision process complex and performance metrics should be chosen so they can be understood by decision-makers and participants [@punt2016].

## STEP 3. SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS

Uncertainties inherent in the underlying system are represented in the OM. These may be related to the biology of the stock (e.g., growth, natural mortality, recruitment, migration), the dynamics of the fleet (e.g., targeting behaviour, selectivity of the fishing gear), the observation process (e.g., bias or imprecision in survey data or age/length composition data), and the implementation process (e.g., exceeding catch limits) [@carruthers2018]. 

Some of this uncertainty (e.g., range of values of natural mortality or other parameters) may be captured within a single OM by expressing distributions for these parameters. However, it is unlikely that the full range of uncertainties thought to influence the system can be captured in one model. Therefore, best practice recommends dividing MSE trials into a "reference set", representing a core set of of OMs that include the most important uncertainties, and a "robustness set", representing other plausible OM formulations that represent alternative structural hypotheses (e.g., time-varying natural mortality, climate-driven recruitment, or changes to available resource monitoring in the future [@rademeyer2007]). @punt2016 provide a list of factors whose uncertainty commonly has a large impact on MSE performance (their Table 3). They note that, in some cases, where the data used to parameterize the OM are in conflict (e.g., indices of abundance are in conflict), the best practice may be to develop alternative OMs based on the different data sources.

Ideally, OMs should be calibrated to real data to ensure they can reproduce historical observations [e.g., @cox2008a; @forrest2018]. In very data-limited cases, of course, this may not be possible. In these cases, best practice would be to develop a set of OMs that differ in terms of major uncertainties, especially related to stock productivity and current depletion level.

## STEP 4. IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES

Management procedures for managing output-controlled fisheries (i.e., fisheries managed by catch limits) are generally either model based, where data are integrated into a stock assessment model and outputs are used as input to a HCR, or empirically based, where data are used in combination with an algorithm to directly determine the catch limit [@punt2016]. Data-limited MPs generally fall into the latter category, although simple stock assessment models such as surplus production models may sometimes be used. 

The scientific literature now reports many MPs for data-limited fisheries, more than 80 of which have been integrated into the DLMtool software [@carruthers2016; @carruthers2018]. Given the large number of MP options available, a screening step is desirable. For example, MPs that do not return a catch limit (e.g., spatial closures or effort-based MPs) can be screened out if management requires a catch limit. After that, trial simulations may be used to screen out MPs that do not meet a basic set of requirements for a broad range of stocks (e.g., MPs that result in a high probability of stocks being in the critical zone). This latter step is an example of "satisficing" [@miller2010], where MPs must meet a minimum standard to be accepted. Satisficing criteria may be used at the screening stage and at the final MP selection stage. 

It is important to test only MPs for which information or data are available [@punt2016]. For example, data-limited MPs that rely on age data or an estimate of current depletion may not be feasible for many BC groundfish stocks. While it is important to work with a managable set of MPs, it is also important not to screen too aggresively, to make sure good candidate MPs are not screened out early on.

## STEP 5. SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES

Once the OM and MPs are fully specified, the MSE simulation trials can be run, following the process illustrated in Figure \@ref(fig:mse-chart). Essentially, all of the MPs are tested in in the simulation framework, using data generated by each OM. Critically, the simulations include feedback between the OM and the MP, where the OM generates data at each time step, which is used to apply the MP, which generates a catch recommendation, which is removed from the OM, which generates the next time step of data, and so forth until the projection period is complete. 

Typically a large number of replicate simulations are run for each OM-MP combination. Replicates may differ in terms of OM process error, observation errors and random draws from ranges of OM parameters, meaning that each replicate provides a different set of simulated data to the MPs. The number of replicates should be selected to ensure that performance metrics can be calculated with adequate precision [@punt2016], which may be indicated by MPs being consistently ranked in the same order throughout the simulated projection period [@carruthers2018]. The MSE should output enough information to calculate performance metrics for the MPs, and also to evaluate the behaviour and performance of the MSE itself (e.g., whether all trials converged, ranges of OM parameter values, and trajectories of key OM variables such as biomass and catch).

## STEP 6. PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE

Selection of the "best" MP involves addressing trade-offs (e.g., between conservation and economic performance metrics), and therefore is the purview of decision-makers, stakeholders, and interested parties [@punt2016]. Ultimately, selection of the best MP may be a subjective process, depending on the magnitude of trade-offs. It may be necessary to rank performance metrics in order of priority before the process starts. The role of science in this step is to ensure that results are clearly presented to decision-makers. Ideally this should include presentation of graphical outputs that enable clear comparison of MPs with respect to performance metrics and trade-offs [@punt2015].

A satisficing step may be used to screen out MPs that did not meet a minimum standard [@miller2010]. After this, MP selection may be an iterative process, where MPs and/or OMs are refined following examination of results [e.g., @cox2008a]. In cases where there is a reference and robustness set of OMs, it may be necessary to weight OMs on the basis of plausibility, although this may require a qualitative, expert-driven approach and may not be straightforward [@punt2016].

@carruthers2018 also discuss a final step, which is formal review of the selected MP once it has been implemented with real data. Formal review includes evaluation of whether the MP is performing as expected by, for example, comparing whether real relative abundance indices follow similar trajectories to those predicted by the MSE. In this document, we do not apply the MPs to real data, but recognize that ongoing review of the performance of MPs following their application is a critical component of MSE, where OMs and MPs may be continuously refined as new data become available [@cox2008a].

# HOW THIS FRAMEWORK IMPLEMENTS MANAGEMENT-PROCEDURE-APPROACH BEST PRACTICES

## STEP 1. DEFINE THE DECISION CONTEXT

Much of the decision context is laid out by the...

The decision to be made on the timeframe for making the decision are laid out in the request for science advice...

The boundaries on the project and decision are often decided on by the technical committee for a given assessment, which is typically comprised of representatives from DFO Science, Management, recreational and commercial industry, First Nations representatives, NGOs, ... other interested parties.

We suggested the technical committee being responsible for proposing a set of objectives and performance metrics for a specific stock. This may require somewhat more consultation then technical committees for traditional stock assessments, but we are not proposing a full MSE process... which require a level of consultation that is not feasible for all Pacific Region groundfish stocks.

As laid out in... decisions on catch limits are ultimately decided by Fisheries Management based on a consensus on science advice from a regional peer review meeting and in consideration of...

We emphasize that the decision to be made in the context of the MP Framework is what MP to choose based on trade-offs in performance amongst MPs. The simulation framework tests the performance of specific MPs---it does not test posthoc adjustments to the catch limit recommended by an MP.

This is in contrast to the traditional decision tables presented in most Pacific Region groundfish stock assessments in recent years.

## STEP 2. SELECTION OF OBJECTIVES AND PERFORMANCE METRICS

* Here we describe a set of provisional objectives and associated performance metrics.
* We call the provisional because they are by no means final but represent a starting point and may be used if there is not reason to change them for a given stock.
* Tie this into the PA Framework.

We propose the following provisional operational/tactical conservation and fisheries objectives:
* Maintain stock status above the LRP in the long-term with extremely high  ability.
* Maintain stock status above the USR...!?
* Avoid overfishing with high probability.
* Given the conservation objectives are achieved, maximize short- and
  long-term fisheries yield.
* Given the conservation objectives are achieved, minimize variability
  in fisheries yield from year to year.
    
* The specific probabilities assigned to successfully achieving each objective likely need some consideration on a stock by stock basis. But we provisionally suggest the following based on the PA Framework and precedents in other DFO Pacific Region assessments...

* We provisionally suggest a 50 year projection.
    * Shorter-lived stocks may use a shorter projection period; longer-live stocks may use a longer projection period.
    * Applications for specific uses like COSEWIC may require a specific projection period.

We propose the following provisional performance metrics, where SB refers to spawning biomass and MSY refers to maximum sustainable yield:

1. LT P40: Probability SB > 0.4 SBMSY (years 36--50)
2. LT P80: Probability SB > 0.8 SBMSY (years 36--50)
3. PNOF: Probability of not overfishing P(F < F[MSY]) (years 1--50)
4. STY: Probability yield > 0.5 MSY (years 6--20)
5. LTY: Probability yield > 0.5 MSY (years 36--50)
6. AAVY: Probability AAVY (average annual variability in yield) < 0.2 (years 1--50)

LT P40, LT P80, and PNOF are conservation performance metrics. LTY, STY, and AAVY are fisheries metrics.

## STEP 3. SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS

* We recommend dividing operating models into reference and robustness sets [@rademeyer2007].

* DLMtool allows for incorporation of uncertainty in most OM parameters by specifying a distribution, typically a uniform distribution between two bounds.

* There can also be value in dividing the uncertainty into specific OMs to understand the applications of different scenarios.

The most important axes of uncertainty in reference sets that are likely across most stocks this framework will apply to include:

* Natural mortality (M)
* Steepness
* Depletion

Candidates for robustness sets may include:

* Predation scenarios (e.g., seal predation)
* Changes in availability of prey
* The effectiveness of or changes to closed areas such as Rockfish Conservation Areas (RCAs)
* Implementation error (actual catches are above or below the TAC)

DLMtool includes a very large number of parameters in the OM that can vary through time or be biased from one stochastic iteration to the next. To simplify the operating model and focus on what are likely the most important axes of uncertainty, we propose fixing a number of parameters to be unbiased and fixed through time unless this is an axis of uncertainty that makes sense to explore for specific stock (Appendix \@ref(app:default-slots)).

In the spirit of reproducible research and transparency, we suggest that the full specification of all parameters in the operating model be clearly documented in an implementation of the MP Framework. We demonstrate this in our case studies within this document (Appendix TODO and Appendix TODO).

*TODO: Abbreviate the following and put many more details into the appendix including the model description*

DLMtool includes an implementation of a Stock Reduction Analysis (REF), which is effectively a catch-at-age or surplus production model (depending on data inputs) that explores the combinations of historical fishing mortality and recruitment would be consistent with the observed data.

Kimura and Tagart (1982)
Walters et al. (2006)

The SRA implementation in MSEtool includes stochastic sampling of the specified operating model parameters. For example, if steepness is defined as being drawn from a uniform distribution between 0.6 and 0.9 across 200 stochastic iterations, then the SRA model will be fit 200 times, each time drawing a different value from that uniform distribution. Those sampled values of steepness are then stored and associated with the estimated parameters in the SRA model such as depletion and annual fishing mortality.

The SRA implementation in MSEtool can be conditioned on catch or effort timeseries. For most applications for groundfish in the Pacific Region, we suggest that conditioning on catch would make the most sense given that we tend to have a better handle on historical trajectories of catch than effort.

If catch are not available going back to unfished conditions, the user can specify a range of initial depletion associated with the first year of available catch.

The SRA function can take as input:

* indices of relative abundance or biomass
* age composition data
* length composition data
* mean length data

Given that in the Pacific Region we tend to have full-length composition data rather than just mean length data, the mean length data input is unlikely to be used within the MP Framework.

Although the SRA requires a complete time series of catch, it can make use of potentially sparse inputs of relative abundance and age or length composition data.

The SRA model can accommodate multiple indices of abundance and multiple fishing fleets.

Selectivity is fixed based on input values in the operating model if there are not any age or length composition data included.

The SRA model conditions the following OM parameters:

* unfished recruitment
* depletion
* relative fishing effort
* annual recruitment deviations
* selectivity parameters if age or length composition data are included

Details on the SRA model are available in Appendix TODO.

## STEP 4. IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES

We screened all MPs available in DLMtool as of November 2019 to consider their appropriateness for the MP Framework (Appendix \@ref(app:MPs)).

In summary, we excluded all MPs that required knowledge of:

* absolute abundance
* recent age composition data
* depletion
* steepness.

We excluded MPs that required knowledge of absolute abundance since they are unlikely to be cases where we have such knowledge in a data limited case. We excluded MPs that required recent age composition data because we intend this framework to be applied to stocks for which recent age competition data are not available. We excluded MPs that required knowledge of depletion and steepness since these are likely to be major axes of uncertainty stocks that the framework will be applied to. While it is necessary to explore these axes of uncertainty within the operating model, implementing an MP on real data would require choosing a value of depletion and steepness, which we argue would be a hard choice to justify and would be challenging to incorporate our true levels of uncertainty about and levels of bias in these parameters. 

DLMtool contains a number of MPs that make catch recommendations based on trends in mean fish length. These may be appropriate for some stocks, but ...

We suggest the empirical index-based MPs are likely to be most suitable for most data-limited groundfish stocks in the Pacific Region. This family of MPs generally try to maintain an index (usually a relative biomass or abundance index from a fisheries-independent survey) at some level by adjusting catch recommendations up or down as the index moves up or down. Different MPs include various levels of smoothing or thresholds and various methods of deriving a target index level. Such MPs have been considered as part of ... in the Pacific Region (REFs).

In addition to the empirical index-based MPs DLMtool and MSEtool include simple surplus production models, which may be considered for some stocks. We emphasize that these model-based MPs are simpler than the implementations often used in previous Pacific Region stock assessments. The point of using these models is to test their success at achieving the performance metrics and not the specific assumptions made within the implementations of the model.

DLMtool currently generates a single index of abundance for use in the data- or model-based MPs. In the future the software may include multiple indices of abundance; however, the vast majority of published data-limited MPs are based on single indices of abundance. This presents a conundrum for groundfish stocks in the Pacific Region since the trawl and hook and line fisheries-independent surveys that cover our coast do so on a biennial basis alternating amongst areas. We suggest the following solutions at the present time:

* Build and test operating models for areas associated with a single index. If these areas are considered simultaneously in a single application of the MP Framework then we suggest comparing performance of the MPs across all areas and, if possible, choosing an MP that performs reasonably well across all areas.

* Develop a single index by "stitching" multiple survey indices together, likely with the application of geostatistical spatiotemporal modeling [REFs].

Important to include reference MPs to bound the range of expected performance under scenarios such as: no fishing, perfect information (F_MSY_ and 0.75 F_MSY), and maintaining the status quo TAC.

### Broad types of MPs:

- Make into a table?

**Constant-catch MPs**

- no feedback!
- often fine for short-term projections
- longterm biomass will go towards equilibrium, which may be far lower (e.g. 0) or higher than desired
- mostly as a point of reference

**Index-ratio MPs**

- moving reference window
- simple concept
- has potential for pathological behaviour since it's not "anchored" to any fixed reference

**Index-slope MPs**

- moving reference window
- similar to "index ratio" types, but fits a regression of some kind and makes recommendations based on the slope

**Index-target MPs**

- fixed reference window
- likely to stabilize B and F and catch over time; at what level is the question
- makes the assumption that the specified target index is where we want the stock to be (which is tested against objectives but conditional on the operating model)
- target window may require judgement on a case-by-case basis

**Model-based MPs**

- here, SP models paired with HCRs
- more complex than data-based rules
- more theoretically sound; doesn't necessarily mean will perform well against objectives
- likely requires "tuning" of priors and possibly HCRs

Thoughts:

- MP development a rich of research and it is probably just in its infancy
- Beyond some minor adjustments to existing MPs, MP development is not the point of this report
- More MPs may be developed as part of the application of this framework (and are likely to be developed elsewhere in the literature) and could be included in the future
- No reason why an MP couldn't combine multiple aspects of the above, although there are advantages to simple and easy to understand MPs
- The MPs described in this report are a provisional library to build from
- MPs are meant to be followed until re-evaluated or the validity of the whole exercise is thrown off (obviously changes may need to be made given unanticipated surprises, but any major changes should also trigger an OM re-evaluation)

## STEP 5. SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES

* Once the OM and MPs are fully specified, DLMtool can be used to repeatedly simulate from the operating model and apply the management procedures.

* Replicates from run to run differ in  ...

* We suggest that sufficient replicates be run until the rank order of the various MPs across the various performance metrics within any given operating model remains consistent.

* We suggest only including replicates with MPs that converged (only applicable to the model-based MPs).

## STEP 6. PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE

For this step we focus on developing a set of provisional visualizations that facilitate comparison of performance metrics across MPs and evaluation of trade-offs amongst them.

Spider plots [REF]. These illustrate performance trade-offs amongst the various MPs in a condensed visual fashion that lends itself to small multiples [REF] to explore a variety of OMs and MPs in a small amount of space.

Visualizations highlight the reference MPs for ease of comparison.

A performance metric Table/Figure. Shaded by probability of achieving that performance metric with rows ordered by performance metric starting with the leftmost performance metric and working across the columns to break ties. We think the shading helps to draw the eye to differences and similarities in performance.

Important to show historical trajectories of parameters such as: historical catch, historical recruitment, historical SSB, historical F, ... and any others that make particular sense for a given stock.

We developed projection plots that show the range of projected values in B/B[MSY], F/F[MSY], catch, and TAC recommendations along with example replicates for the various MPs.

We developed Kobe plots [REF] that illustrate the trade-off between F/F[MSY] and B/B[MSY] across replicates for the various MPs and emphasize the parameters space with the highest density via quantile kernel density estimate contour lines.

