# BEST PRACTICES FOR IMPLEMENTING A MANAGEMENT PROCEDURE APPROACH

A critical first step in any decision-making processes is defining the decision context [@gregory2012]. This is particularly important in complex decision-making environments such as MSE, where it is necessary to clarify the decision that needs to be made and, importantly, the planned approach for reaching a decision([^1]). Some key questions to be answered at this stage are provided under Step 1 below. @punt2016 reviewed best practices for MSE and identified five key steps in the process (described under Steps 2--6 below). In large part, DLMtool has been designed to follow these steps, illustrated in Figure \@ref(fig:mse-chart) [@carruthers2018].

(ref:fig-mse-chart) DRAFT: The steps of the MSE process following @punt2016 as implemented in DLMtool.

```{r mse-chart, fig.cap="(ref:fig-mse-chart)", out.width="6.3in"}
knitr::include_graphics(here::here("report/figure/mse-chart.pdf"))
```

Importantly, decision-makers, stakeholders and other interested parties (First Nations, NGOs, academics) should be engaged throughout the process, particularly in defining the decision context, setting objectives and performance metrics and selection of MPs [e.g., @cox2008a].

<!-- *TO DO: figure out how to cite the landmark report. We are not allowed to cite unpublished reports in CSAS docs (I don't think), but we could include it in a footnote. I tried doing a footnote but it didn't work. Anyway, here is the ref    ([^1]: Cox, S.P., and Benson, A.J. Unpublished. Roadmap to more sustainable Pacific herring fisheries in Canada: a step-by-step guide to the management strategy evaluation approach. Prepared for Pelagics Resource Management, Fisheries and Oceans Canada Pacific Region.) * -->

## STEP 1. DEFINE THE DECISION CONTEXT

Key questions to guide definition of the decision context for MSE include:

* What is the the exact decision to be made?

* What is the timeframe for making the decision?

* What are the boundaries on the project and decision?

* What are specific roles and responsibilities of those involved, who may include science, management, First Nations, industry, academia, and NGOs? 

* How will the final decision be made? For example, it may be necessary to rank or weight objectives if there is large conflict in performance against objectives.

## STEP 2. SELECTION OF OBJECTIVES AND PERFORMANCE METRICS

Before running any MSE analyses, clear objectives and the performance metrics that measure them must be identified. Ideally, these should be developed with the participation of decision-makers, stakeholders, and other interested parties. Objectives may initially be high level and "strategic" (e.g., achieve sustainable fisheries, maintain economic prosperity, maintain cultural access) but these must be converted into operational "tactical" objectives that can be expressed as quantitative performance metrics [@punt2016]. Fully quantified objectives include a metric, the desired probability of success, and a time frame to achieve the objective (e.g., probability that stock size is below LRP is less than 5% over 50 years). Since the underlying system represented by the OM is known exactly, a wide range of biological and economic metrics can be calculated [@carruthers2018]. 

@punt2016 note that care should be taken to avoid having too many performance metrics, which could make the final decision process too complex. They also advise that, to facilitate the decision process, performance metrics should be chosen so they can be understood by decision-makers and participants.

## STEP 3. SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS

Uncertainties inherent in the underlying system are represented in the OM. These may be related to the biology of the stock (e.g., growth, natural mortality, recruitment, migration), the dynamics of the fleet (e.g., targeting behaviour, selectivity of the fishing gear), the observation process (e.g., bias or imprecision in survey data or age/length composition data), and the implementation process (e.g., exceeding catch limits) [@carruthers2018]. 

Some of this uncertainty (e.g., range of values of natural mortality or other parameters) may be captured within a single OM by expressing distributions for these parameters. However, it is unlikely that the full range of uncertainties thought to influence the system can be captured in one model. Therefore, best practice recommends dividing MSE trials into a "reference" set, represented by a baseline OM, and a "robustness" set, represented by other plausible OM formulations that represent alternative structural hypotheses (e.g., time-varying natural mortality vs constant natural mortality, climate-driven recruitment, or alternative depletion states of the stock) [@rademeyer2007]. @punt2016 provide a list of factors whose uncertainty commonly has a large impact on MSE performance (their Table 3). They note that, in some cases, where the data used to parameterize the OM are in conflict (e.g., indices of abundance are in conflict), the best practice may be to develop alternative OMs based on the different data sources.

Ideally, OMs should be calibrated to real data to ensure they can reproduce historical observations [e.g., @cox2008a; @forrest2018]. In very data-limited cases, of course, this may not be possible. In these cases, best practice would be to develop a set of OMs that differ in terms of major uncertainties, especially related to stock productivity and current depletion level.

## STEP 4. IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES

Management procedures for managing output-controlled fisheries (i.e., fisheries managed by catch limits) are generally either model-based, where data are integrated into a stock assessment model and outputs are used as input to a HCR, or empirically-based, where data are used in combination with an algorithm to directly determine the catch limit [@punt2016]. Data-limited MPs generally fall into the latter category, although simple stock assessment models such as surplus production models may sometimes be used. 

The scientific literature now reports many MPs for data-limited fisheries, more than 80 of which have been integrated into the DLMtool software [@carruthers2016; @carruthers2018]. Given the large number of MP options available, a screening step is desirable. For example, MPs that do not return a catch limit (e.g., spatial closures or effort-based MPs) can be screened out. After that, trial simulations may be used to screen out MPs that do not meet a basic set of requirements for a broad range of stocks (e.g., MPs that result in high probability of stocks in the critical zone). This latter step is an example of "satisficing" [@miller2010], where MPs must meet a minimum standard to be accepted. Satisficing criteria may be used at the screening stage and at the final MP selection stage. 

It is important to test only MPs for which information or data are available [@punt2016]. For example, data-limited MPs that rely on age data or an estimate of current depletion may not be feasible for many BC groundfish stocks. While it is important to work with a managable set of MPs, it is also important not to screen too aggresively, to make sure good candidate MPs are not screened out early on.

## STEP 5. SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES

Once the OM and MPs are fully specified, the MSE simulation trials can be run, following the process illustrated in Figure \@ref(fig:mse-chart). Essentially, all of the MPs are tested in in the simulation framework, using data generated by each OM. Critically, the simulations include feedback between the OM and the MP, where the OM generates data at each time step, which is used to apply the MP, which generates a catch recommendation, which is removed from the OM, which generates the next time step of data, and so forth until the projection period is complete. 

Typically a large number of replicate simulations are run for each OM-MP combination. Replicates may differ in terms of OM process error, observation errors and random draws from ranges of OM parameters, meaning that each replicate provides a different set of simulated data to the MPs. The number of replicates should be selected to ensure that performance metrics can be calculated with adequate precision [@punt2016], which may be indicated by MPs being consistently ranked in the same order throughout the simulated projection period [@carruthers2018]. The MSE should output enough information to calculate performance metrics for the MPs, and also to evaluate the behaviour and performance of the MSE itself (e.g., whether all trials converged, ranges of OM parameter values, and trajectories of key OM variables such as biomass and catch).

## STEP 6. PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE

Selection of the "best" MP involves addressing trade-offs (e.g., between conservation and economic performance metrics), and therefore is the purview of decision-makers, stakeholders, and interested parties [@punt2016]. Ultimately, selection of the best MP may be a subjective process, depending on the magnitude of trade-offs. It may be necessary to rank performance metrics in order of priority before the process starts.

The role of science in this step is to ensure that results are clearly presented to decision-makers. Ideally this should include presentation of graphical outputs that enable clear comparison of MPs with respect to performance metrics and trade-offs [@punt2015].

A satisficing step may be used to screen out MPs that did not meet a minimum standard [@miller2010]. After this, MP selection may be an iterative process, where MPs and/or OMs are refined following examination of results [e.g., @cox2008a]. In cases where there is a reference and robustness set of OMs, it may be necessary to weight OMs on the basis of plausibility, although this may require a qualitative, expert-driven approach and may not be straightforward [@punt2016].

@carruthers2018 also discuss a final step, which is formal review of the selected MP once it has been implemented with real data. Formal review includes evaluation of whether the MP is performing as expected by, for example, comparing whether real relative abundance indices follow similar trajectories to those predicted by the MSE. In this document, we do not apply the MPs to real data, but recognize that ongoing review of the performance of MPs following their application is a critical component of MSE, where OMs and MPs may be continuously be refined as new data become available [@cox2008a].

# HOW THIS FRAMEWORK IMPLEMENTS MANAGEMENT-PROCEDURE-APPROACH BEST PRACTICES

## STEP 1. DEFINE THE DECISION CONTEXT

## STEP 2. SELECTION OF OBJECTIVES AND PERFORMANCE METRICS

## STEP 3. SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS

## STEP 4. IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES

## STEP 5. SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES

## STEP 6. PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE
