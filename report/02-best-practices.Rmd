# BEST PRACTICES FOR IMPLEMENTING A MANAGEMENT PROCEDURE APPROACH

A critical first step in any decision-making processes is defining the decision context [@gregory2012]. This is particularly important in complex decision-making environments such as MSE, where it is necessary to clarify the decision that needs to be made and, importantly, the planned approach for reaching a decision. Some key questions to be answered at this stage are provided under Step 1 below. @punt2016 reviewed best practices for MSE and identified five key steps in the process (described under Steps 2--6 below). In large part, DLMtool has been designed to follow these steps, illustrated in Figure \@ref(fig:mse-chart) [@carruthers2018].

(ref:fig-mse-chart) The steps of the MSE process following @punt2016 as implemented in DLMtool. Adapted from @carruthers2018.

```{r mse-chart, fig.cap="(ref:fig-mse-chart)", out.width="6.3in"}
knitr::include_graphics(here::here("report/figure/mse-chart.pdf"))
```

Importantly, decision-makers, stakeholders, and other interested parties (e.g., First Nations, Nongovernmental Organizations [NGOs], and academics) should be engaged throughout the process, particularly in defining the decision context, setting objectives and performance metrics and selection of MPs [e.g., @cox2008a].

<!-- TODO: figure out how to cite the landmark report. We are not allowed to cite unpublished reports in CSAS docs (I don't think), but we could include it in a footnote. I tried doing a footnote but it didn't work. Anyway, here is the ref    ([^1]: Cox, S.P., and Benson, A.J. Unpublished. Roadmap to more sustainable Pacific herring fisheries in Canada: a step-by-step guide to the management strategy evaluation approach. Prepared for Pelagics Resource Management, Fisheries and Oceans Canada Pacific Region.) -->

## STEP 1. DEFINE THE DECISION CONTEXT

Key questions to guide defining the decision context for MSE include:

* What is the the exact decision to be made?

* What is the timeframe for making the decision?

* What are the boundaries on the project and decision?

* What are specific roles and responsibilities of those involved, who may include science, management, First Nations, industry, academia, and NGOs? 

* How will the final decision be made? For example, it may be necessary to rank or weight objectives if there is large conflict in performance against objectives.

## STEP 2. SELECTION OF OBJECTIVES AND PERFORMANCE METRICS

Before running any MSE analyses, clear objectives and the performance metrics that measure them must be identified. Ideally, these should be developed with the participation of decision-makers, stakeholders, and other interested parties. Objectives may initially be high level and "strategic" (e.g., achieve sustainable fisheries, maintain economic prosperity, maintain cultural access) but these must be converted into operational "tactical" objectives that can be expressed as quantitative performance metrics [@punt2016]. Fully quantified objectives include a metric, the desired probability of success, and a time frame to achieve the objective (e.g., the probability that the stock size is below the LRP is less than 5% over 50 years). Since the underlying system represented by the OM is known exactly, a wide range of biological and economic metrics can be calculated [@carruthers2018]. However, having too many performance metrics can make the final decision process complex and performance metrics should be chosen so they can be understood by decision-makers and participants [@punt2016].

## STEP 3. SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS

Uncertainties inherent in the underlying system are represented in the OM. These may be related to the biology of the stock (e.g., growth, natural mortality, recruitment, migration), the dynamics of the fleet (e.g., targeting behaviour, selectivity of the fishing gear), the observation process (e.g., bias or imprecision in survey data or age/length composition data), and the implementation process (e.g., exceeding catch limits) [@carruthers2018]. 

Some of this uncertainty (e.g., range of values of natural mortality or other parameters) may be captured within a single OM by expressing distributions for these parameters. However, it is unlikely that the full range of uncertainties thought to influence the system can be captured in one model. Therefore, best practice recommends dividing MSE trials into a "reference set", representing a core set of of OMs that include the most important uncertainties, and a "robustness set", representing other plausible OM formulations that represent alternative structural hypotheses (e.g., time-varying natural mortality, climate-driven recruitment, or changes to available resource monitoring in the future [@rademeyer2007]). @punt2016 provide a list of factors whose uncertainty commonly has a large impact on MSE performance (their Table 3). They note that, in some cases, where the data used to parameterize the OM are in conflict (e.g., indices of abundance are in conflict), the best practice may be to develop alternative OMs based on the different data sources.

Ideally, OMs should be calibrated to real data to ensure they can reproduce historical observations [e.g., @cox2008a; @forrest2018]. In very data-limited cases, of course, this may not be possible. In these cases, best practice would be to develop a set of OMs that differ in terms of major uncertainties, especially related to stock productivity and current depletion level.

## STEP 4. IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES

Management procedures for managing output-controlled fisheries (i.e., fisheries managed by catch limits) are generally either model based, where data are integrated into a stock assessment model and outputs are used as input to a HCR, or empirically based, where data are used in combination with an algorithm to directly determine the catch limit [@punt2016]. Data-limited MPs generally fall into the latter category, although simple stock assessment models such as surplus production models may sometimes be used. 

The scientific literature now reports many MPs for data-limited fisheries, more than 80 of which have been integrated into the DLMtool software [@carruthers2016; @carruthers2018]. Given the large number of MP options available, a screening step is desirable. For example, MPs that do not return a catch limit (e.g., spatial closures or effort-based MPs) can be screened out if management requires a catch limit. After that, trial simulations may be used to screen out MPs that do not meet a basic set of requirements for a broad range of stocks (e.g., MPs that result in a high probability of stocks being in the critical zone). This latter step is an example of "satisficing" [@miller2010], where MPs must meet a minimum standard to be accepted. Satisficing criteria may be used at the screening stage and at the final MP selection stage. 

It is important to test only MPs for which information or data are available [@punt2016]. For example, data-limited MPs that rely on age data or an estimate of current depletion may not be feasible for many BC groundfish stocks. While it is important to work with a managable set of MPs, it is also important not to screen too aggresively, to make sure good candidate MPs are not screened out early on.

## STEP 5. SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES

Once the OM and MPs are fully specified, the MSE simulation trials can be run, following the process illustrated in Figure \@ref(fig:mse-chart). Essentially, all of the MPs are tested in in the simulation framework, using data generated by each OM. Critically, the simulations include feedback between the OM and the MP, where the OM generates data at each time step, which is used to apply the MP, which generates a catch recommendation, which is removed from the OM, which generates the next time step of data, and so forth until the projection period is complete. 

Typically a large number of replicate simulations are run for each OM-MP combination. Replicates may differ in terms of OM process error, observation errors and random draws from ranges of OM parameters, meaning that each replicate provides a different set of simulated data to the MPs. The number of replicates should be selected to ensure that performance metrics can be calculated with adequate precision [@punt2016], which may be indicated by MPs being consistently ranked in the same order throughout the simulated projection period [@carruthers2018]. The MSE should output enough information to calculate performance metrics for the MPs, and also to evaluate the behaviour and performance of the MSE itself (e.g., whether all trials converged, ranges of OM parameter values, and trajectories of key OM variables such as biomass and catch).

## STEP 6. PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE

Selection of the "best" MP involves addressing trade-offs (e.g., between conservation and economic performance metrics), and therefore is the purview of decision-makers, stakeholders, and interested parties [@punt2016]. Ultimately, selection of the best MP may be a subjective process, depending on the magnitude of trade-offs. It may be necessary to rank performance metrics in order of priority before the process starts. The role of science in this step is to ensure that results are clearly presented to decision-makers. Ideally this should include presentation of graphical outputs that enable clear comparison of MPs with respect to performance metrics and trade-offs [@punt2015].

A satisficing step may be used to screen out MPs that did not meet a minimum standard [@miller2010]. After this, MP selection may be an iterative process, where MPs and/or OMs are refined following examination of results [e.g., @cox2008a]. In cases where there is a reference and robustness set of OMs, it may be necessary to weight OMs on the basis of plausibility, although this may require a qualitative, expert-driven approach and may not be straightforward [@punt2016].

@carruthers2018 also discuss a final step, which is formal review of the selected MP once it has been implemented with real data. Formal review includes evaluation of whether the MP is performing as expected by, for example, comparing whether real relative abundance indices follow similar trajectories to those predicted by the MSE. In this document, we do not apply the MPs to real data, but recognize that ongoing review of the performance of MPs following their application is a critical component of MSE, where OMs and MPs may be continuously refined as new data become available [@cox2008a].

# HOW THIS FRAMEWORK IMPLEMENTS MANAGEMENT-PROCEDURE-APPROACH BEST PRACTICES

## STEP 1. DEFINE THE DECISION CONTEXT

Much of the decision context is laid out by the...

The decision to be made on the timeframe for making the decision are laid out in the request for science advice...

The boundaries on the project and decision are often decided on by the technical committee for a given assessment, which is typically comprised of representatives from DFO Science, Management, recreational and commercial industry, First Nations representatives, NGOs, ... other interested parties.

We suggested the technical committee being responsible for proposing a set of objectives and performance metrics for a specific stock. This may require somewhat more consultation then technical committees for traditional stock assessments, but we are not proposing a full MSE process... which require a level of consultation that is not feasible for all Pacific Region groundfish stocks.

As laid out in... decisions on catch limits are ultimately decided by Fisheries Management based on a consensus on science advice from a regional peer review meeting and in consideration of...

We emphasize that the decision to be made in the context of the MP Framework is what MP to choose based on trade-offs in performance amongst MPs. The simulation framework tests the performance of specific MPs---it does not test posthoc adjustments to the catch limit recommended by an MP.

This is in contrast to the traditional decision tables presented in most Pacific Region groundfish stock assessments in recent years.

## STEP 2. SELECTION OF OBJECTIVES AND PERFORMANCE METRICS

* Here we describe a set of provisional objectives and associated performance metrics.
* We call the provisional because they are by no means final but represent a starting point and may be used if there is not reason to change them for a given stock.
* Tie this into the PA Framework.

We propose the following provisional operational/tactical conservation and fisheries objectives:
    * Maintain stock status above the LRP in the long-term with extremely high probability.
    * Maintain stock status above the USR...!?
    * Avoid overfishing with high probability.
    * Given the conservation objectives are achieved, maximize short- and 
      long-term fisheries yield.
    * Given the conservation objectives are achieved, minimize variability 
      in fisheries yield from year to year.
    
* The specific probabilities assigned to successfully achieving each objective likely need some consideration on a stock by stock basis. But we provisionally suggest the following based on the PA Framework and precedents in other DFO Pacific Region assessments...

* We provisionally suggest a 50 year projection.
    * Shorter-lived stocks may use a shorter projection period; longer-live stocks may use a longer projection period.
    * Applications for specific uses like COSEWIC may require a specific projection period.

We propose the following provisional performance metrics, where SB refers to spawning biomass and MSY refers to maximum sustainable yield:

1. LT P40: Probability SB > 0.4 SBMSY (years 36--50)
2. LT P80: Probability SB > 0.8 SBMSY (years 36--50)
3. PNOF: Probability of not overfishing P(F < F[MSY]) (years 1--50)
4. STY: Probability yield > 0.5 MSY (years 6--20)
5. LTY: Probability yield > 0.5 MSY (years 36--50)
6. AAVY: Probability AAVY (average annual variability in yield) < 0.2 (years 1--50)

LT P40, LT P80, and PNOF are conservation performance metrics. LTY, STY, and AAVY are fisheries metrics.

## STEP 3. SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS

Uncertainties inherent in the underlying system are represented in the OM. These may be related to the biology of the stock (e.g., growth, natural mortality, recruitment, migration), the dynamics of the fleet (e.g., targeting behaviour, selectivity of the fishing gear), the observation process (e.g., bias or imprecision in survey data or age/length composition data), and the implementation process (e.g., exceeding catch limits) [@carruthers2018]. 

Some of this uncertainty (e.g., range of values of natural mortality or other parameters) may be captured within a single OM by expressing distributions for these parameters. However, it is unlikely that the full range of uncertainties thought to influence the system can be captured in one model. Therefore, best practice recommends dividing MSE trials into a "reference set", representing a core set of of OMs that include the most important uncertainties, and a "robustness set", representing other plausible OM formulations that represent alternative structural hypotheses (e.g., time-varying natural mortality, climate-driven recruitment, or changes to available resource monitoring in the future [@rademeyer2007]). @punt2016 provide a list of factors whose uncertainty commonly has a large impact on MSE performance (their Table 3). They note that, in some cases, where the data used to parameterize the OM are in conflict (e.g., indices of abundance are in conflict), the best practice may be to develop alternative OMs based on the different data sources.

Ideally, OMs should be calibrated to real data to ensure they can reproduce historical observations [e.g., @cox2008a; @forrest2018]. In very data-limited cases, of course, this may not be possible. In these cases, best practice would be to develop a set of OMs that differ in terms of major uncertainties, especially related to stock productivity and current depletion level.

* We recommend dividing operating models into reference and robustness sets [@rademeyer2007].
* Major axes of uncertainty that are likely across most stocks this framework will apply to include: natural mortality, depletion, and 

DLMtool allows for incorporation of uncertainty in most OM parameters by specifying a distribution, typically a uniform distribution between two bounds.

There can also be value in dividing the uncertainty into specific OMs to understand the applications of different scenarios.

The most important axes of uncertainty in reference sets for most but not all stocks are likely to include:

* Natural mortality (M)
* Steepness
* Depletion

Candidates for robustness sets may include:

* Predation scenarios (e.g., seal predation)
* Changes in availability of prey
* The effectiveness of or changes to closed areas such as Rockfish Conservation Areas (RCAs)
* Implementation error (actual catches are above or below the TAC)

DLMtool includes a very large number of parameters in the OM that can vary through time or be biased from one stochastic iteration to the next. To simplify the operating model and focus on what are likely the most important axes of uncertainty, we propose fixing a number of parameters to be unbiased and fixed through time unless this is an axis of uncertainty that makes sense to explore for specific stock (Appendix \@ref(app:default-slots)).

## STEP 4. IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES

We screened all MPs available in DLMtool as of November 2019 to consider their appropriateness for the MP Framework (Appendix \@ref(#app:MPs)).

In summary, we excluded all MPs the required as input knowledge of:

* absolute abundance
* depletion
* steepness
* recent age composition data

DLMtool contains a number of MPs that make catch recommendations based on trends in mean fish length. These may be appropriate for some stocks, but ...

We suggest the empirical index-based MPs are likely to be most suitable for most data-limited groundfish stocks in the Pacific Region. This family of MPs generally try to maintain an index (usually a relative biomass or abundance index from a fisheries-independent survey) at some level by adjusting catch recommendations up or down as the index moves up or down. Different MPs include various levels of smoothing or thresholds and various methods of deriving a target index level. Such MPs have been considered as part of ... in the Pacific Region (REFs).

In addition to the empirical index-based MPs DLMtool and MSEtool include simple surplus production models, which may be considered for some stocks. We emphasize that these model-based MPs are simpler than the implementations often used in previous Pacific Region stock assessments. The point of using these models is to test their success at achieving the performance metrics and not the specific assumptions made within the implementations of the model.

DLMtool currently generates a single index of abundance for use in the data- or model-based MPs. In the future the software may include multiple indices of abundance; however, the vast majority of published data-limited MPs are based on single indices of abundance. This presents a conundrum for groundfish stocks in the Pacific Region since the trawl and hook and line fisheries-independent surveys that cover our coast do so on a biennial basis alternating amongst areas. We suggest the following solutions at the present time:

* Build and test operating models for areas associated with a single index. If these areas are considered simultaneously in a single application of the MP Framework then we suggest comparing performance of the MPs across all areas and, if possible, choosing an MP that performs reasonably well across all areas.

* Develop a single index by "stitching" multiple survey indices together, likely with the application of geostatistical spatiotemporal modeling [REFs.


## STEP 5. SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES

## STEP 6. PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE

For this step we focus on developing a set of provisional visualizations that facilitate comparison of performance metrics across MPs and evaluation of trade-offs amongst them.

